---
layout: post
title: "Ищем темы в Игре Престолов"
date: 2015-09-13 14:54:20 +0300
comments: true
categories: 
author: Alexander Senov
description: Основы тематического моделирования на примере текста книг серии "Песнь Льда и Пламени"
keywords: visualization
---


Когда мы имеем дело с большим количеством текстовых документов, первое что нас интересует --- о чем эти документы: есть ли между ними что-то общее, о чем каждый из документов, о чем они в целом? Попробуем ответить на эти вопросы воспользовавшись инструментарием науки о данных. Да не просто так, а на примере серии книг "Песнь Льда и Пламени", известной также как "Игра Престолов".

{% img /images/2015-09_TopicModelling_GameOfThrones/GameOfThrones_tag_cloud.jpg 768 576 КДПВ %}

<!-- more -->

Одно из основных понятий, которое может помочь отметить на заданные вопросы ---  *тема*. Тема --- это то, о чем говориться в тексте, например, объект его обсуждения. Разумеется, любой может определить основную тему текста, прочитав его. Однако, есть несколько причин поступить иначе. Во-первых, этот процесс достаточно трудоемкий, особенно если учесть потенциально большие коллекции документов (например, весь интернет). Во-вторых, этот подход субъективный, а хотелось бы получить объективную (а еще лучше, численную) характеристику тематики текста. Наконец, этот блог не о чтении, а об анализе данных. И именно путем анализа данных мы и планируем пойти.

Для ответа на наши вопросы мы воспользуемся таким подходом, как *тематическое моделирование*. Тематическое моделирование --- это подраздел машинного обучения, заключающийся в построении *тематической модели* по заданному текстовому корпусу. В свою очередь, тематическая модель --- это статистическая модель, которая моделирует взаимосвязь наблюдамемых переменных: слов и документов и ненаблюдаемых переменных --- тем, отвечая тем самым на два основных вопроса:

1. Какие слова образуют каждую из тем?
2. К каким темам относится каждый из документов?

В этой статье мы разберемся в алгоритме LSA --- одном из базовых алгоритмов тематического моделирование, а реализуем его на языке Python, а главное --- применим его на корпусе, составленном из книг серии "Песнь Льда и Пламени". Соответственно, статья состоит из трех частей: теории, практики и обсуждения результатов. Первая часть может вызвать приступ "T.L.D.R.", поэтому если вы знакомы с основами тематического моделирования или же в них не заинтересованы, то можете сразу переходить ко второй части. Если же интересно узнать лишь что получилось --- смело прокручивайте до третьей части.


# Часть 1. Теория

В это части мы сначала опишем несколько алгоритмов, а затем покажем как объединив их вместе можно получить алгоритм тематического моделирования LSA.

Итак, определившись с тем что мы хотим получить (тематическую модель), остается понять, как же это сделать. Сразу скажу, без математики не обойтись. К сожалению, математике не знакомы понятия "текст", "документ", "слово", "тема" и т.п. Зато, ей хорошо понятен язык чисел и векторов. И именно на язык векторов нам и предстоит перейти для решения нашей задачи. Для этого мы воспользуемся так называемой *векторной моделью* текста --- "переведем" наш корпус с прикладного языка слов и документов на абстрактный язык линейной алгебры.


## Векторная модель
Векторная модель --- модель представления текстовых документов в виде числовых векторов, где каждое размерность вектора-документа соответствует какому-либо слову. Рассмотрим построение этой модели.

На входе мы имеем коллекцию из $$N$$ документов. Пронумеруем их числами от 1 до $$N$$, так что индексом $$d \in \{1..N\}$$ обозначается $$d$$-й элемент коллекции. Далее, рассмотрим словарь слов --- все уникальные слова, встречающиеся в наших документах. Допустим, таких слов $$M$$ штук и пронумеруем их от 1 до $$M$$ индексом $$w$$. Теперь, посчитаем сколько раз каждое слово $$w=1..M$$ входит входит в каждый документ $$d=1..N$$ и обозначим это число $$n_{d,w}$$. Из этих чисел сформируем матрицу $$\mathbf{X} = (n_{d,w})_{d,w}$$ --- [матрицу частот слов в документах](https://en.wikipedia.org/wiki/Document-term_matrix). Строки этой матрицы соответствуют документам, а столбцы --- словам. Скорее всего, эта матрица будет *разряженной* --- большая часть ее элементов будет равна нулю (ведь каждый документ содержит лишь небольшую долю всех слов).

Перевод окончен! Теперь наш текстовый корпус представлен в виде матрицы $$\mathbf{X}$$. 
В результате получится матрица выглядящая примерно следующим образом.

{% img /images/2015-09_TopicModelling_GameOfThrones/GameOfThrones_X_matrix_vis.svg 384 288 Матрица документ-слово %}

Стоит заметить, что подобная модель под собой имеет два основных предположения.

- Порядок документов в коллекции не имеет значения.
- Порядок слов в документе не имеет значения (т.н. "мешок слов" или "bag of words").

Если первое предположение вполне естественно, то второе может показаться неоднозначным. Казалось, даже такая мелочь, как запятая в предложении "казнить нельзя, помиловать" полностью меняет его смысл, что уж говорить о порядке слов даже не в одном предложении, а в нескольких абзацах. Несмотря на это резонное замечание, модель "мешка слов" --- одна из наиболее широко используемых и хорошо зарекомендовавшая себя на практике. Тем более это верно для такой задачи, как определение темы. Действительно, как не расставляй слова и знаки препинания в предложении "казнить, нельзя помиловать", легко понять что речь идет о казни ("казнить" же или "помиловать" --- детали).


## Стемминг
Далее развивая мысль о значимости тех или иных деталей для определения тематики текста, можно заметить, что слово может встречаться в тексте в различных формах: с различными окончаниями и приставками, --- но с единым смыслом. Например, неважно какую форму слова мы встретили в тексте: "казнить", "казнят", "казнен", "казню", "казнишь", "казни" --- все они относятся к теме "казнь". Именно на нахождение [основы слова](https://ru.wikipedia.org/wiki/%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D0%B0_%D1%81%D0%BB%D0%BE%D0%B2%D0%B0) по той или иной заданной его форме и направлен такой инструмент, как *стемминг*. 
Здесь мы не будем разъяснять, какие бывают алгоритма стемминга (а их довольно много) и как они работают. Если возникнет интерес, можно начать со 
[статьи в википедии](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%B5%D0%BC%D0%BC%D0%B8%D0%BD%D0%B3). Скажем только, что в дальнейшем мы будем использовать [стеммер Snowball](http://snowball.tartarus.org/), а точнее [его реализацию в NLTK](http://www.nltk.org/_modules/nltk/stem/snowball.html).

Имея в руках стеммер, можно использовать его для преобразования каждого слова каждого документа в его нормальную форму. Тем самым мы, во-первых, значительно уменьшим общее число слов, а следовательно и вычислительную трудоемкость, а во-вторых, упростим дальнейшую интерпретацию результатов.	


## TF-IDF
TF-IDF --- (Term Frequency - Inverse Document Frequency) методика оценки важности слова в документе. Она опирается на два основных предположения

1. Частота появления слова в документе пропорциональна его важности в этом документе.
2. Число документов, в котором встречается слово обратно пропорционально его важности.

Первое предположение вполне логично, а если поразмыслить то и с обоснованием второго не возникнет проблем: возьмем, например, "и" или "а" --- они наверняка встретятся в большинстве документов, но на вряд ли привносят что-то в их тематику. Другой пример: возьмем текст новостей, посвященных гражданину Н-му. Естественно, 99.9% из них будут содержать его фамилию в той или иной форме, которая в то же время, будет совершенно бесполезна для определение их темы (в контексте общей темы, посвященной этому гражданину).

Эти два предположения TFIDF учитывает с помощью функций  $$\mathrm{tf}$$ и $$\mathrm{idf}$$ соответственно. Задаются они следующим образом

- $$\mathrm{tf}(w, d) = n_{w, d}$$ --- число вхождений слова $$w$$ в документ $$d$$;
- $$\mathrm{idf}(w) = \log \frac{N}{\lvert \{d \; : \; n_{w, d} > 0\} \rvert }$$ --- логарифм обратной доли документов, содержащих слово $$w$$.  

Итоговая же оценка важности слова $$w$$ для документа $$d$$ описывается функцией $$\mathrm{tfidf}$$: 

$$
\mathrm{tfidf}(w, d) = \mathrm{tf}(w, d) * \mathrm{idf}(w)
$$

Таким образом, меняя $$\mathbf{X} = (n_{d,w})_{d,w}$$ на матрицу $$X_{tfidf} = (\mathrm{tfidf}(w, d))_{d, w}$$, мы понижаем важность слов, встречающихся в большинстве документов и повышаем ее у слов встречающихся в относительно небольшом подмножестве документов.


## Сингулярное разложение

Потихоньку мы подбираемся к самому интересному. Как же найти ответы на наши вопросы касательно тем?
Напомним, на какие вопросы должна ответить искомая тематическая модель коллекции документов.

1. Какие слова образуют каждую из тем?
2. К каким темам относится каждый из документов?

Наша задача --- численно ответить на эти вопросы на том же языке, на котором описана матрица документов-слов $$\mathbf{X}$$. 

Рассмотрим конкретную тему $$t$$ и ответы на эти вопросы в векторном виде:

1. Вектор $$u_t \in \mathrm{R}^{N }$$, $$d$$-й элемент которого $$u_t^{(d)}$$ символизирует близость темы $$t$$ документу $$d$$.
2. Вектор $$v_t \in \mathrm{R}^{M }$$, $$w$$-й элемент которого $$v_t^{(w)}$$ символизирует важность слова $$w$$ для темы $$t$$. 

Заметим, что если слово $$w$$ важно для темы $$t$$ ($$v_t^{(w)}$$ велико), а тема $$t$$ близка документу $$d$$ ($$u_t^{(d)}$$ велико), то велико будет и их произведение: $$u_t^{(d)} v_t^{(w)}$$. Если же тема $$t$$ близка документу $$d$$, а слово $$w$$, напротив, не играет роли в теме $$t$$ ($$v_t^{(w)} \sim 0$$), то и их произведение будет близко к нулю: $$u_t^{(d)} v_t^{(w)}\sim 0$$.   Более того, если перемножить два этих вектора, то получившаяся матрица $$X_t = u_t v_t^T$$ будет ни чем иным как корпус с единственной темой --- темой $$t$$. 

Развивая эту идею, задачу построения тематической модели можно сформулировать следующим образом: найти комбинацию тем $t=1..K$ и соответствующих им векторов $$u_t$$ и $$v_t$$, таких что их комбинация наилучшим образом описывает исходный корпус $$\mathbf{X}$$. "Наилучшим образом" будем понимать в смысле наименьшего квадратичного отклонения:

$$
	\lvert\lvert X - \sum\limits_{t=1}^K u_t v_t^\top \rvert\rvert_2 \longrightarrow \min\limits_{\{u_t, v_t\}_{t=1}^K}.
$$ 

Здесь на сцену выходит *сингулярное разложение* (*singular value decomposition*, *SVD*), решающее схожую задачу. Оно заключается в представлении вещественной матрицы $$X \in \mathrm{R}^{N\times M}$$, $$N > M$$ в виде:

$$
X = U S V^\top = \sum_{t=1}^{M} s_t u_t v_t^\top,
$$
 
где $$U \in \mathrm{R}^{N\times M}$$, $$V \in \mathrm{R}^{M\times M}$$ --- ортогональные матрицы, состоящие из левых ($$u_t$$) и правых ($$v_t$$) сингулярных векторов, а $$S \in \mathrm{R}^{M\times M}$$ --- диагональная матрица, на главной диагонали которой находятся сингулярные числа ($$s_k$$).

Для наглядности мы проиллюстрировали неполное сингулярное разложение матрицы документ-слово (крестиками обозначены ненулевые значения).

{% img /images/2015-09_TopicModelling_GameOfThrones/GameOfThrones_SVD_vis.svg 768 576 SVD матрицы документ-слово %}

Сингулярное разложение --- одно из важнейших матричных разложений, применяемая во множестве как теоретических, так и практических областей: нахождении псевдообратной матрицы, решении линейных уравнений, снижении размерности, анализе временных рядов, рекомендательных системах и др. В качестве отправной точки можно обратится к [википедии](https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D0%BD%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5) или довольно наглядной статье на [ams.org](http://www.ams.org/samplings/feature-column/fcarc-svd).


SVD обладает множеством полезных слов, в контексте нашей задачи определения тем в корпусе нам интересны следующие:

1. Длинна правых сингулярных векторов равна единице: $$v_t = 1$$ --- это означает, что все вектора тем ($$v_t$$) лежат на единичной окружности в пространстве слов, тем самым находясь в едином масштабе, и все что их отличает друг от друга --- это "угол поворота" --- то какие слова для них более важные, а какие менее;
2. Длинна левых сингулярных векторов равна единице: $$u_t = 1$$ --- вектора документов ($$u_t$$) лежат на единичной окружности в пространстве тем, аналогично находясь в едином масштабе, и их отличие заключается в пропорциях тех или иных тем;
3. Все вектора $$\{v_t\}$$ ортогональны друг другу --- исключена возможность того, что все темы будут друг на друга похожи --- в пространстве слов они ортогональны друг другу;
4. Сингулярные числа $$s_t$$ расположены на диагонали матрицы $$S$$ по убыванию --- сперва идут темы обладающие наибольшим вкладом в коллекцию;
5. Сингулярные вектора определены с точность до знака: одновременно домножив $$u_t$$ и $$v_t$ на -1 ничего не изменится --- это значит, что у каждой темы есть, фактически два полюса: один описывается словами с наибольшим положительным весом, а другой --- с наибольшим отрицательным;
6. Если рассмотреть *сокращенное сингулярное разложение* (*truncated SVD*): $$X_K = \sum_{t=1}^{K} s_t u_t v_t^\top $$, то это будет *наилучшим приближением матрицы $$X$$ ранга $$K$$ (в терминах $$\lvert\lvert.\rvert\rvert_2$$ нормы). Это означает, что любой другой набор из $$K$$ тем, представленный в виде троек $$\{u_t, s_t, v_t\}_1^K$$ будет хуже описывать наш исходный корпус.

Довольно-таки неплохо! Учитывая, что это достается нам совершенно бесплатно :) 


Подытожим. Имея матрицу $$X$$, все что нам нужно сделать для получения его тематической модели --- это выбрать число $$K < N, M$$ и воспользоваться SVD:

$$
	U_K, S_K, V^\top_K = \mathrm{svd}(X, K)
$$

и тогда каждую тройку $$u_k, s_k, v_k$$ можно будет интерпретировать следующим образом:

- $$u_t \in \mathrm{R}^{N}$$ --- вектор соответствия темы $$t$$ каждому из документов $$d=1..N$$, чем больше $$u_t^{(d)}$$ --- тем ближе документ $$d$$ к теме $$t$$;
- $$v_t \in \mathrm{R}^{M}$$ --- вектор соответствия слов $$w=1..M$$ теме $$t$$, чем больше $$v_t^{(w)}$$ --- тем важнее слово $$w$$ в теме $$t$$;
- $$s_t \in \mathrm{R}$$ --- относительный вес темы $$t$$ в корпусе.

Стоит отметить, что согласно пятому свойству, "больше" стоит понимать в абсолютном смысле, ведть большое отрицательно число можно легко превратить в большое положительно, домножив соответствующие вектора $$u_t, v_t$$ на -1.


## Латентный Семантический Анализ

На этом с математикой покончено! Осталось собрать элементы мозайки воедино. 

Латентный семантический анализ фактически является комбинацией описанных ваше методов. Кратко алгоритм его можно описать следющим образом.

0. На входе LSA поступает коллекция текстовых документов.
1. Текстовые документы переводятся в матрицу частот слов в документах $$X$$ посредством векторной модели.
2. Элементы матрицы $$X$$ взвешиваются посредством TF-IDF: $$X_{tfidf} = \mathrm{tfidf}(X)$$.
3. К взвешенной матрице применяется SVD: $$U_K, S_K, V^\top_K = \mathrm{svd}(X_{tfidf}, K)$$.
4. Полученные тройки $$u_t, s_t, v_t$$ используются для интерпретаций тем $$t=1..K$$.

Как видно, среди этапов алгоритма отсутствует стемминг. Тем не менее, эта операция является де-факто стандартом в задачах тематического моделирования и его мы добавили по собственной инициативе в следующем разделе (можно рассмотреть его в качестве шага алгоритма под номером $$\frac{1}{2}$$).

На этом с теорией наконец-то покончено, перейдем к практике!


# Часть 2. Практика


С чего начать? С получения данных, конечно! Нам нужен текст Игры Престолов, желательно всех вышедших книг. Есть различные схемы, в том числе черные и серые, но есть и белые. На правах рекламы, мы воспользовались совершенно белым предложением интернет-магазина litres.ru, где можно приобрести всю серию по [довольно привлекательной цене](http://www.litres.ru/serii-knig/pesn-lda-i-ognya/elektronnie-knigi/) --- после этого все книги будут доступны в множестве форматов, в том числе и предпочтительным для нас txt.

Когда книги скачены, можно перейти первому этапу --- предобработке данных

##  Предобработка данных

Сначала, разберем текст книг по главам и посмотрим на их размер по числу слов.

На примере первой книги "Игры Престолов":

```text Игра_Престолов
0. Пролог 2925
1. Бран 2295
2. Кейтилин 1643
3. Дейенерис 3284
4. Эддард 3068
...
68. Дейенерис 3273
69. Тирион 2738
70. Джон 3909
71. Кейтилин 3641
72. Дейенерис 2738
```

и последней на данный момент книге серии --- 2-го тома "Танца с Драконами":

```text Танец_с_Драконами_2
0. Принц Винтерфелла 4564
1. Страж 3891
2. Джон 2532
3. Тирион 3033
4. Переметчивый 3441
...
31. Укротитель драконов 2584
32. Джон 3897
33. Десница королевы 4106
34. Дейенерис 3775
35. Эпилог 4518
```

Судя по списку глав все верно, идем дальше. Посмотрим на сам текст --- возьмем на вскидку несколько абзацев (точнее, блоков текста, разделенных переносом строки): 

```text Пример блоков текста
Оша поглядела на него.
------------------------------
– И ты ее ненавидишь?
------------------------------
– Н-но вы проповедуете материнское милосердие…
------------------------------
Все хорошо, Грейджой. Слышишь, какая тишь? Тебе бы прыгать от радости. Ты взял Винтерфелл меньше чем с тридцатью людьми – такой подвиг достоин песен. Сейчас он вернется в постель, перевернет Киру на спину и возьмет ее снова, чтобы прогнать призраки. Ее вздохи и смешки рассеют застывшую тишину.
------------------------------
– Это ваш долг.
```

Как видно, здесь есть как довольно осмысленные абзацы, но много и обрывок фраз, вырванных из контекста. Вряд ли, например, фраза "Оша поглядела на него" полезна для определение темы. Эту проблему недостатка контекста мы решим просто: объединим каждые три последующих текстовых блока вместе:

```text Пример объединенных блоков текста
Оша поглядела на него.
– Ты просил, и они ответили. Открой уши, прислушайся, сам услышишь.
Бран прислушался.
------------------------------
– И ты ее ненавидишь?
– Почти так же сильно, как люблю. Прошу извинить меня, моя королева, – я очень устал.
Дени отпустила его, но, когда он уже собрался выйти, она не удержалась и спросила:
```

--- уже лучше! Теперь все готово: мы преобразовали исходные тексты в "документы" --- объединенные блоки текста. Можно применять LSA.

## Перевод в векторную модель
Как мы помним, первый этап LSA --- перевод документов векторную модель. 
Начнем с разбиения наших документов на слова и их стемминг

```python Разбиение документов на слова
import re
non_letter_rgxp = re.compile(u'[^а-яА-Я ]') 

fix_doc = lambda doc: non_letter_rgxp.sub(' ', doc.lower()) 

import nltk
from nltk.stem.snowball import SnowballStemmer

stemmer = SnowballStemmer("russian")

def stem(tokens):
    return (stemmer.stem(t) for t in tokens)

docs_tokens = [
    list(filter(lambda w: w, stem(remove_non_letters(doc.lower()).split(' '))))
    for doc in docs
]
```

Теперь, посчитаем частоту слов

```python Подсчет частоты слов
import collections

token_frequency_dict = collections.defaultdict(lambda: 0)
for tokens in docs_tokens:
    for t in tokens:
        token_frequency_dict[t] += 1
```

Взглянем на наиболее часто встречающиеся слова

```text Самые частые слова
400392 и
271847 он
261481 не
239817 в
187947 на
134724 с
129038 что
```

.., и на самые редкие

```text Самые редкие слова
3 линнистер
3 близя
3 персонаж
3 нервнич
3 свежеоперен
3 прожиг
3 долженствова
```

Как видно, среди часто встречающихся слов довольно много бессмысленных "коротышек": "а", "и", "не" и т.п. От редких же слов больше вреда, чем пользы: они раздувают словарь слов (а значит и размерность будущей матрицы $$X$$, делая вычисления более сложными), а в определении темы вряд ли помогут, так как встречаются в считанном числе документов. 

Решено! Отфильтруем самые редкие слова, а так же слова маленькой длинны:

```python Фильтрация корпуса по словам
docs_tokens_filtered = [
    filter(lambda t: token_frequency_dict[t] > 5 and len(t) > 2, tokens) 
    for tokens in docs_tokens
]
```

Остается перевести наши разбитые на слова и отфильтрованные документы в векторный вид. Следующий блок кода делает именно это.

```python Перевод текстовых документов в матрицу частот документ-слов
import itertools
import scipy as sp

def flatten(iterators_iterator):
    return itertools.chain.from_iterable(iterators_iterator)

all_tokens = set(flatten(paragraphs_tokens_filtered))

id_token_dict = dict(enumerate(all_tokens))
token_id_dict = dict(((v, k) for k, v in id_token_dict.items()))
       
def doc2vec(doc_tokens, token_id_dict):
    id_cnt_dict = collections.Counter((token_id_dict[t] for t in doc_tokens))
    return list(id_cnt_dict.items())

def docs2csr_matrix(docs_tokens, token_id_dict):
    docs_vecs = [doc2vec(doc_tokens, token_id_dict) for doc_tokens in docs_tokens]
    data = list(flatten((((id_cnt[1] for id_cnt in doc_vec) for doc_vec in docs_vecs))))
    row_ind = list(flatten((((doc_ind for id_cnt in doc_vec) for doc_ind, doc_vec in enumerate(docs_vecs)))))
    col_ind = list(flatten((((id_cnt[0] for id_cnt in doc_vec) for doc_vec in docs_vecs))))
    return sp.sparse.csr_matrix((data, (row_ind, col_ind)), dtype=float)
    

X = docs2csr_matrix(docs_tokens_filtered)
```

Ура, мы в векторе! Получилась матрица 48977 на 27673, идем дальше.

## TFIDF
Следующим по списку стоит TFIDF. Воспользуемся собственной реализацией.

```python TFIDF
def tfidf(X):
    idf = np.log((X.shape[0] + 1) * 1. / ((X > 0).sum(0)) + 1) + 1.
    idf = sp.sparse.spdiags(np.array(idf)[0], diags=0, m=X.shape[1], n=X.shape[1])
    return X * idf

X_tfidf = tfidf(X)
```

## Применение SVD
На этот раз свой велосипед писать не будем, воспользуемся готовой реализацией для разряженных матриц (а нас как-раз такая) из пакета [scipy](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html).

```python SVD
U, S, VT = sp.sparse.linalg.svds(X_tfidf, k=40)
```

Вот и все, готово! Давайте посмотрим, какие темы нашел LSA.

# Часть 3. Результаты

Взглянем на сингулярные числа $$s_t$$ соответствующую вкладу каждой тему в коллекцию

{% img /images/2015-09_TopicModelling_GameOfThrones/GameOfThrones_singular_values_histogram.png 768 576 %}

Первое собственное число всех стоит одинокой башней. Неужели есть какая-та настолько "выдающаяся" тема?  
Как мы говорили выше, элементы вектора $$v_t$$ соответствуют вкладу соответствующих слов в тему $$t$$. Посмотрим же на самые большие по модулю элементы вектора $$v_0$$. А для наглядности  рядом с каждым значением припишем соответствующее слово, представив их в формате "вес"*"слово".

```text Первый правый собственный вектор
Тема №1: +0.268*что +0.224*как +0.196*был +0.173*сво +0.147*лорд +0.145*так +0.131*все +0.125*когд +0.115*сир +0.102*сказа +0.099*чтоб +0.097*есл +0.095*котор +0.095*сам +0.094*друг +0.093*джон +0.089*корол +0.089*рук +0.089*тольк +0.086*больш
```

Удивительно, что все элементы одного знака (мы все-все проверили). Получается такая тема, которой соответствуют все слова без исключения. Но если подумать, то ничего удивительного в этом нет. Вспомним базовую статистику. Если взять множество чисел, какое число будет минимизировать сумму квадратов расстояний от них? Правильно --- их среднее. И здесь та же история: фактически, вектор $$v_0$$ --- это среднее по строкам матрицы $$X_{tfidf}$$, то есть вектор средних весов слов в нашем корпусе. Исходя из этого, первая собственная тройка с точки зрения определения темы нам мало полезна, так что отбросим ее и вновь взглянем на график собственных чисел.

{% img /images/2015-09_TopicModelling_GameOfThrones/GameOfThrones_singular_values_histogram_without_1st.png 768 576 %}

Теперь сильно выделяющихся тем нет. Далее пойдем по порядку, рассмотрим слова, образующие темы со 2-й по 7-ю.

### Тема 2. "Власть?"

{% img /images/2015-09_TopicModelling_GameOfThrones/RobertBaratheon_in_Winterfell.jpg 768 576 Старки встречают Роберта Баратеона %}


```text  Наиболее важные слова 2-ой темы
Тема №2: +0.27*лорд -0.23*джон +0.15*корол +0.15*ваш -0.15*стен +0.15*сир +0.14*что -0.13*черн +0.13*мне +0.12*мен -0.12*бран -0.11*сэм +0.11*есл -0.10*был -0.10*дерев +0.10*роберт -0.09*под +0.09*теб -0.08*ног -0.08*ден
```

Похоже, что Джон --- самый "важный" герой серии, во всяком случае с точки зрения LSA :). 
Вторую тему можно интерпретировать двумя способами. Во-первых, эта тема посвящена властьимущим: лордам, королям, сирам, --- об этом говорят и местоимения "ваш", "мне", "мен". Если 
взглянуть на [самые близкие к этой теме документы](https://gist.github.com/Obus/e7cf9223fa7bdc4cda5f) (близость к документа к теме определяется на основе матрицы $$US$$), то можно заметить, что ближе всего к этой теме относятся документы, в которой речь идет о лордах: Ренли и Станисе Баратеонах, лорде Тайвине, лорде-главнокомандующем, об общем лорде-отце, о бесчисленных лордах меньшего размаха, или же  чем-нибудь королевском, например, о короле Роберте, короле Станнисе, королевской деснице, королевской гвардии, и даже о Королевском лесе. Во-вторых, если рассмотреть эту тему "наоборот", домножив вектора $$u_2$$ и $$v_2$$ на -1 (пользуемся свойством SVD номер 5), то ее можно интерпретировать как тему анархии. Так среди соответствующих ей [документов](https://gist.github.com/Obus/ead759846f8a69d51854) можно встретить множество относящихся к проделкам Арьи Старк и похождениям Джона Сноу с одичалыми. 


### Тема 3. "Джон Сноу и его друзья?"

{% img /images/2015-09_TopicModelling_GameOfThrones/JonSnow_NW_trainee.jpg 768 576 Джон Сноу, тренировка в Ночном Дозоре %}

```text  Наиболее важные слова 3-ей темы
Тема №3: +0.38*джон -0.32*сир +0.23*сэм +0.22*что +0.15*теб -0.15*ден -0.15*рыцар +0.15*бран -0.13*тирион -0.12*золот +0.11*мне -0.10*джейм +0.09*так -0.09*меч +0.09*есл -0.09*красн +0.09*одичал +0.08*мен +0.08*дозор +0.08*мейстер
```

Третья тема вполне очевидная: она посвящена Джону Сноу, по большей части эпизодам с участием Сэма Тарли, в чем можно убедиться взглянув на [ее документы](https://gist.github.com/Obus/53fea013e8a368fd94cd).
Большой отрицательный вес у слова "сир" легко объясним: к кому в ночном дозоре можно так обратиться?

Взглянем на следующую тему


### Тема 4. "Северные Лорды"? или "Матерь Драконов"

{% img /images/2015-09_TopicModelling_GameOfThrones/RobbSark_Whispering_Wood_Crop2.png 384 256 Северные Лорды %}
{% img /images/2015-09_TopicModelling_GameOfThrones/MotherOfDragons.jpg 384 256 Матерь Драконов %}

```text Наиболее важные слова 4-ой темы
Тема №4: +0.41*лорд -0.31*ден +0.30*джон -0.20*что +0.18*сир -0.15*дракон -0.14*теб -0.12*так +0.12*робб +0.11*корол -0.11*кхал -0.11*мне -0.10*мен -0.10*дрог +0.10*кейтилин -0.10*как +0.09*стен +0.09*старк +0.08*меч +0.07*станнис
```

--- вновь о Джоне! На этот раз, в отличие от второй темы, "джон" не противопоставляется "лорду". Наоборот, [документы относящиеся к этой теме](https://gist.github.com/Obus/d1bf933cf83323bab21a) довольно часто описывают либо эпизоды из жизни лордов: лорда главнокомандующего, Робба Старка в окружении северных лордов и прочих. Примечательно, что Дейнерис ("ден") в этой теме находится в "противоположном углу" этой темы --- лордов в ее окружении совсем немного. [Документы противоположного угла](https://gist.github.com/Obus/b137a78c9b876759f3bc) целиком посвящены Дейнерис: здесь и "дракон"ы и "кхал" и "дрог"о. Особенно превалирует тема драконов.


На очереди 5-я тема


### Тема 5. "Винтерфелл" или "Джон и Сэм"

{% img /images/2015-09_TopicModelling_GameOfThrones/game-of-thrones-starks-in-winterfell.png 768 576 Старки в Винтерфелле %}

```text Наиболее важные слова 5-ой темы
Тема №5: -0.55*бран -0.31*ходор +0.28*джон +0.19*сэм -0.16*робб -0.15*санс -0.13*был +0.10*станнис +0.10*давос -0.10*кейтилин -0.09*лет -0.09*винтерфелл +0.08*дракон -0.08*что +0.08*корабл +0.08*одичал -0.08*рикон -0.08*волк +0.07*черн -0.07*лювин
```

Эту тему так же можно интерпретировать двояко. Один из полюсов - это полюс Винтерфелла: "бран", "ходор", "робб", "санс", "кейтилин" и другие --- это указывает на события происходящие в замке еще до начала войны. С другой стороны, нельзя не отметить, что пара "бран" и "ходор" превалируют: если вглянуть на [документы этого полюса](https://gist.github.com/Obus/ff4f142a39ea5d18f2f1), то можно убедиться что среди них довольно много относящихся к путешествию Брана и Ходора за стену.  На другом полюсе это еще одна тема "посвященная" Сэму Тарли и Джону Сноу. От третьей темы ее отличает отрицание всего что связано с Винтерфеллом: слов "бран", "робб", "санс", "кейтилин", "винтерфелл" и др., при этом ориентированность на событиям после победы над одичалыми: на то указывают слова "станнис" и "давос". Документы этого полюса доступны по [ссылке](https://gist.github.com/Obus/6863dd57ccd7ba245017).


### Тема 6. "Боевая"

{% img /images/2015-09_TopicModelling_GameOfThrones/Aria_vs_Sirio.jpeg 768 576 Учебный поединок Арьи Старк с Сирио Форелем  %}

```text Наиболее важные слова 6-ой темы
Тема №6: -0.25*тирион -0.23*сир -0.18*джон +0.18*лорд -0.18*рук -0.16*меч -0.16*санс +0.15*ден +0.15*дракон +0.15*давос -0.15*теб -0.14*джейм +0.14*бран +0.12*мор +0.12*корол -0.12*сэм +0.11*корабл -0.11*удар +0.10*станнис +0.09*бог
```

В этой теме наиболее интересна ее "обратная сторона" (полученная умножением на единицу). Здесь превалируют такие слова, как "тирион", "сир", "джон", "рук", "меч" и т.д. Если взглянуть на соответствующие ей [документы](https://gist.github.com/Obus/0fab1293f682d800e9e2), то окажется что эта тема боя, поединок: в нее попадает сражение от лица Тириона на Черноводной, битва Сэма с мертвым Малышом Паулом, поединок между Бронном и сиром Вардисом Игеном в Орлином Гнезде, бой Джона Сноу с Костяным Лордом и многие другие. Таким образом, ключевыми здесь оказываются слова "рук", "меч", "удар" и т.п., а веса отдельных персонажей лишь указывают на их участие в сражениях.

И наконец, 7-я тема

### Тема 7. "Битва при Черноводной" или "Джон Сноу и Дейнерис Таргириен?"

{% img /images/2015-09_TopicModelling_GameOfThrones/TIrion_Blackwater_Angry.jpg 768 576 Тирион в битве при Черноводной  %}

```text Наиболее важные слова 7-ой темы
Тема №7: -0.35*ден -0.32*джон +0.29*тирион -0.25*сир +0.17*давос -0.17*кхал -0.15*дрог +0.15*как +0.13*корабл -0.10*ваш -0.10*брат +0.10*лорд -0.10*корол -0.10*бран +0.10*сэм -0.09*джор -0.09*нед -0.09*дракон +0.09*вод -0.08*был
```

С одной стороны, эта тема --- еще одна тема касающаяся сражения, но теперь вполне конкретного: битвы при Черноводной. Об этом можно догадаться взглянув на основны слова: "тирион" (со стороны обороняющихся), "давос" (со стороны нападавших), "корабль" и "вод". Совсем очевидно это становиться, если посмотреть на [соответствующие этой "стороне" темы документы](https://gist.github.com/Obus/f3b80aa271a46cafb044) --- все они относятся к битве у Черноводной. 

Если посмотреть с другой стороны, то это довольно таки загадочная тема, которую попалам делят Джон Сноу и Дейнерис Таргариен: с одной стороны здесь "ден", "кхал" и "дрого", а с другой "джон", "бран" и "нед". Если взглянуть на [соответствующие документы](https://gist.github.com/Obus/c615dc92dc99c350ec1c), то можно заметить, что практически все они содержат слово "сир": что со стороны Дейнерис, что со стороны Джона, а так же часто общим является слово "брат": на севере братьев не счесть, а на юге это брат Дейнерис --- пока еще живой Визерис.



# Заключение

Если вас заинтересовал тема тем (прошу прощения за дурной каламбур) в Игре Престолов, то по [ссылке](https://gist.github.com/Obus/a6b40e31e9a9535eb757) доступны по 20 наиболее важных тем для первых 42 тем. Если же хочется поиграться с темами самостоятельно, то в качестве отправных точек могу посоветовать следующее

1. [IPython Notebook](не готово еще) с кодом, используемым в этой статье, и полученными результатами.
2. Python пакет [gensim](https://radimrehurek.com/gensim/), содержащий как вспомогательный инструменты для создания корпуса, реализацию LSA, так и реализации гораздо более сложных, но и интересных методов тематического моделирования
3. [Статья](http://www.machinelearning.ru/wiki/index.php?title=%D0%A2%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5) на machinelearning.ru

Надеюсь, что было интересно :) 

До встречи!