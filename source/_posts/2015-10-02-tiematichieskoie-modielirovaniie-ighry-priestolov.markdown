---
layout: post
title: "Тематическое моделирование Игры Престолов"
date: 2015-10-02 23:59:59 +0300
comments: true
categories: [Машинное обучение, Тематическое моделирование, Обработка текста, Визуализация]
author: Alexander Senov
description: Основы тематического моделирования на примере текста книг серии "Песнь Льда и Пламени"
keywords: visualization, machine learning, topic modelling, game of thrones, latent semantic analysis, natural language processing, text analysis, singular value decomposition
---


<div>
  <style type="text/css">

    ul{margin:1em 0 1em 2em;}
    ol{margin:1em 0 1em 2em;}

  </style>
</div>


<script src="http://d3js.org/d3.v2.min.js?2.10.0"></script>
<script type="text/javascript" src="/d3/d3.layout.cloud.js"></script>
<script type="text/javascript" src="/d3/d3.topic.pos.neg.clouds.js"></script>




Когда мы имеем дело с большим количеством текстовых документов, первое что нас интересует --- о чем эти документы: есть ли между ними что-то общее, о чем каждый из документов, о чем они в целом? Попробуем ответить на эти вопросы, воспользовавшись инструментарием науки о данных. Да не просто так, а на примере серии книг "Песнь Льда и Пламени" (кратко, ПЛиО), по мотивом которой снят не безызвестный сериал "Игра Престолов".

{% img /images/2015-09_TopicModelling_GameOfThrones/GameOfThrones_tag_cloud.jpg 768 576 КДПВ %}

<!-- more -->

Одно из основных понятий, которое поможет ответить на заданные вопросы ---  *тема*. Тема --- это то, о чем говориться в тексте, например, объект его обсуждения. Разумеется, любой может определить основную тему текста, прочитав его. Однако, есть несколько причин поступить иначе. Во-первых, этот процесс достаточно трудоемкий, особенно если учесть потенциально большие коллекции документов (например, весь интернет) --- одному человеку все не прочитать. Во-вторых, этот подход субъективный, а хотелось бы получить объективную (а еще лучше, численную) характеристику тематики текста. Наконец, этот блог не о чтении, а об анализе данных. И именно путем анализа данных мы и планируем пойти.

Для автоматического поиска тем в документах мы воспользуемся таким подходом, как *тематическое моделирование*. Тематическое моделирование --- это подраздел машинного обучения, изучающий способы построения *тематических моделей* по заданному текстовому корпусу. В свою очередь, тематическая модель --- это статистическая модель, которая моделирует взаимосвязь наблюдамемых переменных: слов и документов и ненаблюдаемых переменных --- тем. Причем, темы строятся (ищутся) автоматически, без участия пользователя (который может лишь задать некоторые параметры модели, например количество тем). 

Тем самым, тематическая модель дает ответы на два основных вопроса:

1. Какие слова образуют каждую из тем?
2. К каким темам относится каждый из документов?

Причем, тематическая модель отвечает на эти вопросы численно: 

1. Для каждого слова и каждой темы дается численная характеристика важности слова для этой темы.
2. Для каждой темы и каждого документа дается численная характеристика, характериизующая роль темы в этом документе.

Таким образом, тема фактически задается весами составляющих ее слов. Более того, слова и документы описываются численным вектором в пространстве тем и..., не будем забегать вперед :)

В этой статье мы разберемся в одном из базовых методов тематического моделирования --- Латентном Семантическом Анализе (aka, Latent Semantic Analysis, LSA, Latent Semantic Indexing, LSI), затем реализуем его на языке Python, а главное --- применим на текстовом корпусе, составленном из книг серии "Песнь Льда и Пламени". Таким образом, статья состоит из трех частей: теории, практики и результатов. Первая часть не отличается краткостью и может вызвать приступ "T.L.D.R.", поэтому, если вы знакомы с основами тематического моделирования или же в них не заинтересованы, можете сразу переходить ко второй части. Если же вас интересуют лишь результаты --- смело прокручивайте до последней.


# Часть 1. Теория

Сначала опишем несколько алгоритмов и подходов, из которых состоит алгоритм тематического моделирования LSA, а затем объеденим их вместе.

Итак, определившись с тем, что мы хотим получить (тематическую модель), остается понять, как же это сделать. Сразу скажу, без математики не обойтись. К сожалению, математике не знакомы понятия "текст", "документ", "слово", "тема" и т.п. Зато, ей хорошо понятен язык чисел и векторов. И именно на язык векторов нам и предстоит перейти для решения нашей задачи. Для этого мы воспользуемся так называемой *векторной моделью* текста --- "переведем" наш корпус с прикладного языка слов и документов на абстрактный язык линейной алгебры.


## Векторная модель
Векторная модель --- модель представления текстовых документов в виде векторов, где каждое измерение вектора-документа соответствует какому-либо слову. Рассмотрим способ построения этой модели.

Итак, на входе у нас коллекция из $$N$$ документов. Пронумеруем их числами от 1 до $$N$$ так,  что индексом $$d \in \{1..N\}$$ обозначается $$d$$-й элемент коллекции. Далее, рассмотрим словарь слов --- это все уникальные слова, встречающиеся в наших документах. Допустим, таких слов $$M$$ штук и пронумеруем их от 1 до $$M$$ индексом $$w$$. Теперь, посчитаем сколько раз каждое слово $$w=1..M$$ входит в каждый документ $$d=1..N$$ и обозначим это число $$n_{d,w}$$. Из этих чисел сформируем матрицу $$\mathbf{X} = (n_{d,w})_{d,w}$$ --- [матрицу частот слов в документах](https://en.wikipedia.org/wiki/Document-term_matrix). Строки этой матрицы соответствуют документам, а столбцы --- словам. Скорее всего, эта матрица будет *разряженной* --- большая часть ее элементов будет равна нулю (ведь каждый документ содержит лишь небольшую долю всех слов).

Перевод окончен! Теперь наш текстовый корпус представлен в виде матрицы $$\mathbf{X}$$. 
В результате получится матрица выглядящая примерно следующим образом.

{% img /images/2015-09_TopicModelling_GameOfThrones/GameOfThrones_X_matrix_vis.svg 384 288 Матрица документ-слово %}

Стоит заметить, что подобная модель под собой имеет два основных предположения.

- Порядок документов в коллекции не имеет значения.
- Порядок слов в документе не имеет значения (т.н. модель "мешка слов" или "bag of words").

Если первое предположение вполне естественно, то второе может показаться неоднозначным. Казалось, даже такая мелочь, как запятая в предложении "казнить нельзя, помиловать" полностью меняет его смысл, что уж говорить о порядке слов даже не в одном предложении, а в нескольких абзацах. Несмотря на это резонное замечание, модель "мешка слов" --- одна из наиболее широко используемых и хорошо зарекомендовавшая себя на практике. Тем более это верно для такой задачи, как определение темы. Действительно, как не расставляй слова и знаки препинания в предложении "казнить, нельзя помиловать", легко понять что речь идет о казни ("казнить" же или "помиловать" --- детали).


## Стемминг
Далее развивая мысль о значимости тех или иных деталей для определения тематики текста, можно заметить, что слово может встречаться в тексте в различных формах: с различными окончаниями и приставками, --- но с единым смыслом. Например, неважно какую форму слова мы встретили в тексте: "казнить", "казнят", "казнен", "казню", "казнишь", "казни" --- все они относятся к теме "казнь". Именно на нахождение [основы слова](https://ru.wikipedia.org/wiki/%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D0%B0_%D1%81%D0%BB%D0%BE%D0%B2%D0%B0) по той или иной заданной его форме и направлен такой инструмент, как *стемминг*. 
Здесь мы не будем разъяснять, какие бывают алгоритма стемминга (а их довольно много) и как они работают. Если возникнет интерес, можно начать со 
[статьи в википедии](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%B5%D0%BC%D0%BC%D0%B8%D0%BD%D0%B3). Скажем только, что в дальнейшем мы будем использовать [стеммер Snowball](http://snowball.tartarus.org/), а точнее [его реализацию в NLTK](http://www.nltk.org/_modules/nltk/stem/snowball.html).

Имея в руках стеммер, можно использовать его для преобразования каждого слова каждого документа в его нормальную форму. Тем самым мы, во-первых, значительно уменьшим общее количество слов, а следовательно и вычислительную трудоемкость, а во-вторых, упростим дальнейшую интерпретацию результатов.	


## TF-IDF
TF-IDF --- (Term Frequency - Inverse Document Frequency) методика оценки важности слова в документе. Она опирается на два основных предположения

1. Частота появления слова в документе пропорциональна его важности в этом документе.
2. Число документов, в котором встречается слово, обратно пропорционально его важности.

Первое предположение вполне логично, а если поразмыслить то и с обоснованием второго не возникнет проблем: возьмем, например, "и" или "а" --- они наверняка встретятся в большинстве документов, но на вряд ли привносят что-то в их тематику. Другой пример: возьмем текст новостей, посвященных гражданину N. Естественно, 99.9% из них будут содержать его фамилию в той или иной форме, которая в то же время, будет совершенно бесполезна для определение их темы (в контексте общей темы, посвященной этому гражданину).

Эти два предположения TFIDF учитывает с помощью функций  $$\mathrm{tf}$$ и $$\mathrm{idf}$$ соответственно. Задаются они следующим образом

- $$\mathrm{tf}(w, d) = n_{w, d}$$ --- число вхождений слова $$w$$ в документ $$d$$;
- $$\mathrm{idf}(w) = \log \frac{N}{\lvert \{d \; : \; n_{w, d} > 0\} \rvert }$$ --- логарифм обратной доли документов, содержащих слово $$w$$.  

Итоговая же оценка важности слова $$w$$ для документа $$d$$ описывается функцией $$\mathrm{tfidf}$$: 

$$
\mathrm{tfidf}(w, d) = \mathrm{tf}(w, d) * \mathrm{idf}(w)
$$

Таким образом, заменяя $$\mathbf{X} = (n_{d,w})_{d,w}$$ на матрицу $$\mathbf{X}_{tfidf} = (\mathrm{tfidf}(w, d))_{d, w}$$, мы понижаем важность слов, встречающихся в большинстве документов и повышаем ее у слов встречающихся в относительно небольшом подмножестве документов.


## Сингулярное разложение

Потихоньку мы подбираемся к самому интересному. Как же найти ответы на поставленные вопросы?
Напомним, на какие вопросы должна ответить искомая тематическая модель коллекции документов.

1. Какие слова образуют каждую из тем?
2. К каким темам относится каждый из документов?

Наша задача --- численно ответить на эти вопросы на том же языке, на котором описана матрица документов-слов $$\mathbf{X}$$. 

Рассмотрим конкретную тему $$t$$ и представи ответы на вопросы выше (пока гипотетически) в векторном виде:

1. Вектор $$u_t \in \mathrm{R}^{N }$$, $$d$$-й элемент которого $$u_t^{(d)}$$ символизирует близость темы $$t$$ документу $$d$$.
2. Вектор $$v_t \in \mathrm{R}^{M }$$, $$w$$-й элемент которого $$v_t^{(w)}$$ символизирует важность слова $$w$$ для темы $$t$$. 

Заметим, что если слово $$w$$ важно для темы $$t$$ ($$v_t^{(w)}$$ велико), а тема $$t$$ близка документу $$d$$ ($$u_t^{(d)}$$ велико), то велико будет и их произведение: $$u_t^{(d)} v_t^{(w)}$$. Если же тема $$t$$ близка документу $$d$$, а слово $$w$$, напротив, не играет роли в теме $$t$$ ($$v_t^{(w)} \sim 0$$), то и их произведение будет близко к нулю: $$u_t^{(d)} v_t^{(w)}\sim 0$$.   Более того, если перемножить два этих вектора, то получившаяся матрица $$]mathbf{X}_t = u_t v_t^T$$ будет ни чем иным как корпус с единственной темой --- темой $$t$$. 

Развивая эту идею, задачу построения тематической модели можно сформулировать следующим образом: 

> Необходимо найти комбинацию тем $t=1..K$ и соответствующих им векторов $$u_t$$ и $$v_t$$, таких что их комбинация наилучшим образом описывает исходный корпус $$\mathbf{X}$$. 

"Наилучшим образом" будем понимать в смысле наименьшего квадратичного отклонения:

$$
	\lvert\lvert X - \sum\limits_{t=1}^K \mathbf{X}_t^\top \rvert\rvert_2 
	 =
	\lvert\lvert X - \sum\limits_{t=1}^K u_t v_t^\top \rvert\rvert_2 
	\longrightarrow 
	\min\limits_{\{u_t, v_t\}_{t=1}^K}.
$$ 

Здесь на сцену выходит *сингулярное разложение* (*singular value decomposition*, *SVD*), решающее схожую задачу. Оно заключается в представлении вещественной матрицы $$\mathbf{X} \in \mathrm{R}^{N\times M}$$, $$N > M$$ в виде:

$$
\mathbf{X} = \mathbf{U} \mathbf{S} \mathbf{V}^\top = \sum_{t=1}^{M} s_t u_t v_t^\top,
$$
 
где $$\mathbf{U} \in \mathrm{R}^{N\times M}$$, $$\mathbf{V} \in \mathrm{R}^{M\times M}$$ --- ортогональные матрицы, состоящие из левых ($$u_t$$) и правых ($$v_t$$) сингулярных векторов, а $$\mathbf{S} \in \mathrm{R}^{M\times M}$$ --- диагональная матрица, на главной диагонали которой находятся сингулярные числа ($$s_k$$).

Сингулярное разложение --- одно из важнейших матричных разложений, применяемое во множестве как теоретических, так и практических областей: нахождении псевдообратной матрицы, решении линейных уравнений, снижении размерности, анализе временных рядов, рекомендательных системах и др. В качестве отправной точки можно обратиться к [википедии](https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D0%BD%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5) или довольно наглядной статье на [ams.org](http://www.ams.org/samplings/feature-column/fcarc-svd).


SVD обладает множеством полезных свойтсва. В контексте нашей задачи определения тем в корпусе нам интересны следующие:


1. Все вектора $$\{v_t\}$$ имеют длину равную единице и ортогональны друг другу --- значит исключена возможность того, что все темы будут друг на друга похожи --- в пространстве слов они ортогональны друг другу;
2. Сингулярные числа $$s_t$$ расположены на диагонали матрицы $$S$$ по убыванию --- сперва идут темы обладающие наибольшим вкладом в коллекцию;
3. Сингулярные вектора определены с точность до знака: одновременно домножив $$u_t$$ и $$v_t$$ на -1 ничего не изменится --- значит у каждой темы есть фактически два полюса: один описывается словами (элементами $$\{v_t\}$$) с наибольшим положительным весом, а другой --- с наибольшим отрицательным;
4. Если рассмотреть *сокращенное сингулярное разложение* (*truncated SVD*): $$X_K = \sum_{t=1}^{K} s_t u_t v_t^\top $$, то это будет *наилучшим приближением матрицы $$X$$ ранга $$K$$* (в терминах $$\lvert\lvert.\rvert\rvert_2$$ нормы). Это означает, что любой другой набор из $$K$$ тем, представленный в виде троек $$\{u_t, s_t, v_t\}_1^K$$ будет хуже описывать наш исходный корпус.

Довольно-таки неплохо! Учитывая, что это достается нам совершенно бесплатно :) 


Подытожим. Имея матрицу $$\mathbf{X}$$, все что нам нужно сделать для получения его тематической модели --- это выбрать число $$K < N, M$$ и воспользоваться truncated SVD:

$$
	U=\mathbf{U}_K, \mathbf{S}_K, \mathbf{V}^\top_K = \mathrm{svd}(\mathbf{X}, K)
$$

и тогда каждую тройку $$u_k, s_k, v_k$$ можно будет интерпретировать следующим образом:

- $$u_t \in \mathrm{R}^{N}$$ --- вектор соответствия темы $$t$$ каждому из документов $$d=1..N$$, чем больше $$u_t^{(d)}$$ --- тем ближе документ $$d$$ к теме $$t$$;
- $$v_t \in \mathrm{R}^{M}$$ --- вектор соответствия слов $$w=1..M$$ теме $$t$$, чем больше $$v_t^{(w)}$$ --- тем важнее слово $$w$$ в теме $$t$$;
- $$s_t \in \mathrm{R}$$ --- относительный вес темы $$t$$ в корпусе.


Для наглядности мы проиллюстрировали сокращенное сингулярное разложение матрицы документ-слово (крестиками обозначены ненулевые значения).

{% img /images/2015-09_TopicModelling_GameOfThrones/GameOfThrones_SVD_vis.svg 768 576 SVD матрицы документ-слово %}

Стоит отметить, что согласно пятому свойству, "больше" стоит понимать в абсолютном смысле, ведть большое отрицательно число можно легко превратить в большое положительно, домножив соответствующие вектора $$u_t, v_t$$ на -1.


## Латентный Семантический Анализ

На этом с математикой покончено! Осталось собрать элементы мозайки воедино. 

Латентный семантический анализ фактически является комбинацией описанных ваше методов. Кратко алгоритм можно описать следющим образом.

0. На входе LSA поступает коллекция текстовых документов.
1. Текстовые документы переводятся в матрицу частот слов в документах $$X$$ посредством векторной модели.
2. Элементы матрицы $$\mathbf{X}$$ взвешиваются посредством TF-IDF: $$\mathbf{X}_{tfidf} = \mathrm{tfidf}(\mathbf{X})$$.
3. К взвешенной матрице применяется SVD: $$\mathbf{U}_K, \mathbf{S}_K, \mathbf{V}^\top_K = \mathrm{svd}(\mathbf{X}_{tfidf}, K)$$.
4. Полученные тройки $$u_t, s_t, v_t$$ используются для интерпретаций тем $$t=1..K$$.

Как видно, среди этапов алгоритма отсутствует стемминг. Тем не менее, эта операция является де-факто стандартом в задачах тематического моделирования и его мы добавили по собственной инициативе в следующем разделе (можно рассмотреть его в качестве шага алгоритма под номером $$\frac{1}{2}$$).

На этом с теорией наконец-то покончено, перейдем к практике!



# Часть 2. Практика


С чего начать? С получения данных, конечно! Нам нужен текст серии "Песнь Льда и Пламени", желательно всех вышедших книг. Есть различные схемы, в том числе черные и серые, но есть и белые. Мы воспользовались совершенно белым предложением интернет-магазина litres.ru (на правах рекламы :)), где можно приобрести всю серию по [довольно привлекательной цене](http://www.litres.ru/serii-knig/pesn-lda-i-ognya/elektronnie-knigi/) --- после этого все книги будут доступны в множестве форматов, в том числе и предпочтительным для нас txt.

Когда книги скачены, можно перейти первому этапу --- предобработке данных.

##  Предобработка данных

Сначала, разберем текст книг по главам и посмотрим на их размер по числу слов.

На примере первой книги "Игре Престолов":

```text Игра_Престолов
0. Пролог 2925
1. Бран 2295
2. Кейтилин 1643
3. Дейенерис 3284
4. Эддард 3068
...
68. Дейенерис 3273
69. Тирион 2738
70. Джон 3909
71. Кейтилин 3641
72. Дейенерис 2738
```

и последней на данный момент книге серии --- 2-го тома "Танца с Драконами":

```text Танец_с_Драконами_2
0. Принц Винтерфелла 4564
1. Страж 3891
2. Джон 2532
3. Тирион 3033
4. Переметчивый 3441
...
31. Укротитель драконов 2584
32. Джон 3897
33. Десница королевы 4106
34. Дейенерис 3775
35. Эпилог 4518
```

Судя по списку глав все верно --- текст мы распарсили правильно, двигаемся дальше. Далее нам нужна коллекция документов. В данном случае документы напрашиваются сами собой: возьмем главы каждой книги. Итого у нас получится 345 документов --- не так уже и много, но документы внушительных размеров.

Теперь все готово: мы преобразовали исходные тексты в "документы" --- объединенные блоки текста. Можно применять LSA.

## Перевод в векторную модель
Как мы помним, первый этап LSA --- перевод документов в векторный вид. 
Начнем с разбиения наших документов на слова и последующий их стемминг

```python Разбиение документов на слова
import re
non_letter_rgxp = re.compile(u'[^а-яА-Я ]') 

fix_doc = lambda doc: non_letter_rgxp.sub(' ', doc.lower()) 

import nltk
from nltk.stem.snowball import SnowballStemmer

stemmer = SnowballStemmer("russian")

def stem(tokens):
    return (stemmer.stem(t) for t in tokens)

docs_tokens = [
    list(filter(lambda w: w, stem(remove_non_letters(doc.lower()).split(' '))))
    for doc in docs
]
```

Теперь, посчитаем частоту слов

```python Подсчет частоты слов
import collections
token_frequency_dict = collections.defaultdict(lambda: 0)
for tokens in docs_tokens:
    for t in tokens:
        token_frequency_dict[t] += 1
```

Взглянем на наиболее часто встречающиеся слова

```text Самые частые слова
400392 и
271847 он
261481 не
239817 в
187947 на
134724 с
129038 что
```

.., и на самые редкие

```text Самые редкие слова
3 линнистер
3 близя
3 персонаж
3 нервнич
3 свежеоперен
3 прожиг
3 долженствова
```

Как видно, среди часто встречающихся слов довольно много бессмысленных "коротышек": "а", "и", "не" и т.п. От редких же слов больше вреда, чем пользы: они раздувают словарь слов (а значит и размерность будущей матрицы $$X$$, делая вычисления более сложными), а в определении темы вряд ли помогут, так как встречаются в считанном числе документов. 

Решено! Отфильтруем самые редкие слова, а так же слова маленькой длины:

```python Фильтрация корпуса по словам
docs_tokens_filtered = [
    filter(lambda t: token_frequency_dict[t] > 5 and len(t) > 2, tokens) 
    for tokens in docs_tokens
]
```

Остается перевести наши разбитые на слова и отфильтрованные документы в векторный вид. Следующий блок кода делает именно это.

{% codeblock lang:python Перевод текстовых документов в матрицу частот документ-слов %}
import itertools
import scipy as sp
def flatten(iterators_iterator):
    return itertools.chain.from_iterable(iterators_iterator)

all_tokens = set(flatten(paragraphs_tokens_filtered))

id_token_dict = dict(enumerate(all_tokens))
token_id_dict = dict(((v, k) for k, v in id_token_dict.items()))
       
def doc2vec(doc_tokens, token_id_dict):
    id_cnt_dict = collections.Counter((token_id_dict[t] for t in doc_tokens))
    return list(id_cnt_dict.items())

def docs2csr_matrix(docs_tokens, token_id_dict):
    docs_vecs = [doc2vec(doc_tokens, token_id_dict) for doc_tokens in docs_tokens]
    data = list(flatten((((id_cnt[1] for id_cnt in doc_vec) for doc_vec in docs_vecs))))
    row_ind = list(flatten((((doc_ind for id_cnt in doc_vec) for doc_ind, doc_vec in enumerate(docs_vecs)))))
    col_ind = list(flatten((((id_cnt[0] for id_cnt in doc_vec) for doc_vec in docs_vecs))))
    return sp.sparse.csr_matrix((data, (row_ind, col_ind)), dtype=float)
    

X = docs2csr_matrix(docs_tokens_filtered)
{% endcodeblock %}


Ура, мы в векторе! Получилась матрица 345 на 11467, идем дальше.

## TFIDF

Следующим по списку стоит TFIDF. Воспользуемся собственной реализацией.

{% codeblock lang:python TFIDF %}
def tfidf(X):
    idf = np.log((X.shape[0] + 1) * 1. / ((X > 0).sum(0)) + 1) + 1.
    idf = sp.sparse.spdiags(np.array(idf)[0], diags=0, m=X.shape[1], n=X.shape[1])
    return X * idf

X_tfidf = tfidf(X)
{% endcodeblock %}


## Применение SVD
На этот раз свой велосипед писать не будем, воспользуемся готовой реализацией для разряженных матриц (а нас как-раз такая) из пакета [scipy](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html).


{% codeblock lang:python SVD %}
U, S, VT = sp.sparse.linalg.svds(X_tfidf, k=40)
{% endcodeblock %}

Вот и все, готово! Давайте посмотрим, какие темы нашел LSA.

# Часть 3. Результаты

Сперва взглянем на сингулярные числа $$s_t$$ соответствующую вкладу каждой тему в коллекцию

{% img /images/2015-09_TopicModelling_GameOfThrones/GameOfThrones_singular_values_histogram.png 768 576 %}

Первое собственное число стоит одинокой башней. Неужели есть какая-та настолько "выдающаяся" тема?  
Как мы говорили выше, элементы вектора $$v_t$$ соответствуют вкладу соответствующих слов в тему $$t$$. Посмотрим же на самые большие по модулю элементы вектора $$v_0$$. Для наглядности мы изобразили облаком наиболее соответствующие теме слова (топ 100 по абсолютному значению $$v_0^{(w)}$$), где размер слова --- его вклад в тему, а цвет --- его знак ("-" - зеленый, а "+" - красный)

<div id="topic_0">
	<svg id='topic_0_pos'></svg>
	<svg id='topic_0_neg'></svg>
</div>
<script type="text/javascript"> 
	topic_clouds('/data/2015-10-01-Game_Of_Thrones_Topic_Modelling/GoT_topic_words_' + 0 + '.csv', 'topic_0_pos', 'topic_0_neg')
</script>


Кажется удивительным, что все элементы одного знака, ни одного отрицательного! Получается такая тема, которой соответствуют все слова без исключения. Но если подумать, то ничего удивительного в этом нет. Вспомним базовую статистику. Если взять множество чисел, какое число будет минимизировать сумму квадратов расстояний от них? Правильно --- их среднее. И здесь та же история: фактически, вектор $$v_0$$ --- это среднее по строкам матрицы $$X_{tfidf}$$, то есть вектор средних весов слов в нашем корпусе. По этой же причине у этой темы и столь выдаяющаесе собственной число. Исходя из этого, первая собственная тройка с точки зрения определения темы нам мало полезна, так что отбросим ее и вновь взглянем на график собственных чисел.

{% img /images/2015-09_TopicModelling_GameOfThrones/GameOfThrones_singular_values_histogram_without_1st.png 768 576 %}

Теперь сильно выделяющихся тем нет. Далее пойдем по порядку, рассмотрим слова, образующие темы со 1-й по 7-ю (рассмотренную "среднюю" тему считаем за нулевую).

## Темы по документам

Собственно, что мы ожидаем увидеть, рассматривая главы в качестве документов? Очевидно, что никакой конкретики здесь получить не удастся --- главы большие и модель мешка слов стирает всю конкретику, перемешивая в кучу всех героев, события и прочее. Поэтому, наиболее вероятный результат --- это что-то, что больше всего различает главы между собой: действующие лица, персонажи, локации. 

Для наглядности мы будем визуализировать наиболее важные слова каждой темы облаком тэгов, где размер слова пропорционален его важности. При этом, для каждой темы облако будет два: одно для слов с положительным кладом (его будем рисовать зеленым цветом) и одно для слов с отрицательным (соответственно, красным). А для некоторых еще посмотрим на соответствующие ей документы.

### Тема 1. За Стеной vs Королевская Гавань

{% img /images/2015-09_TopicModelling_GameOfThrones/GoT_topic_1.png 768 576 Основные слова темы 1 %}


Ого! Почти все самые важные слова темы, что с отрицательные, что положительные --- это имена тех или иных персонажей. Похоже наша догадка по поводу того, что лежит в основе различия документов оказалась не далека от истины :). Попробуем проинтерпретировать как "положительную", так и "отрицательную" часть темы.

Как видно, "положительная" часть посвящена преимущественно Джону и Сэму ("джон" и "сэм"  --- самые ярко выраженные слова темы), а так же их похождениям по обе стороны от стены: об этом говорят такие слова, как "одичал", "крастер", "лилл", "черн" и прочие. Так же сюда затесалось немного "ходора" и "брана", видимо из-за особенностей местности --- и одичалые и ходор с браном большую часть времени провели за стеной, а значит и слова описывающие местность у них совпадают (например, "снег", или "волк") :). 

"Обратная" же тема, судя по словам, соответствует Тириону, а так же другим событиям, относящимся к королевской гавани --- этим можно объяснить столь высокий вклад слова "сир", а так же других ее обитателей: "петир", "джейм", "серсе", "джофф" и т.п.

Соответствующие этой теме главы лишь подтверждают наши выводы:

```text Документы темы 1
Книга 4, Глава 6, Сэмвел
Книга 3, Глава 20, Сэмвел
Книга 2, Глава 24, Джон
Книга 3, Глава 35, Сэмвел
Книга 5, Глава 8, Джон
Книга 1, Глава 53, Джон
Книга 4, Глава 27, Сэмвел
Книга 3, Глава 48, Сэмвел
Книга 3, Глава 17, Джон
Книга 3, Глава 77, Сэмвел
Книга 1, Глава 27, Джон
Книга 3, Глава 57, Джон
```

```text Документы темы 1 "наоборот"
Книга 3, Глава 62, Тирион
Книга 1, Глава 63, Тирион
Книга 3, Глава 68, Тирион
Книга 3, Глава 21, Тирион
Книга 2, Глава 42, Тирион
Книга 1, Глава 39, Тирион
Книга 2, Глава 4, Тирион
Книга 1, Глава 32, Тирион
Книга 4, Глава 28, Джейме
Книга 5, Глава 28, Тирион
Книга 3, Глава 69, Джейме
Книга 4, Глава 25, Серсея
Книга 3, Глава 6, Тирион
```


### Тема 2. Бес vs Луковый рыцарь
<div id="topic_2">
	<svg id='topic_2_pos'></svg>
	<svg id='topic_2_neg'></svg>
</div>
<script type="text/javascript"> 
	topic_clouds('/data/2015-10-01-Game_Of_Thrones_Topic_Modelling/GoT_topic_words_' + 2 + '.csv', 'topic_2_pos', 'topic_2_neg')
</script>

Здесь "позитивная" тема похоже на тему Тириона. Сюда же "затесался" Джон: по сюжету Тирион и Джон Сноу пересекались в 1-ой книге, в Винтерфелле, а так же по дороге и на самой Стене. 
Но, если взглянуть на cоответствующие документы, то помимо 1-ой главы можно увидеть, например и 5-ую. Как объяснить 5-ую книгу серии? Дело в том, что Джон в книге не один: в 5-ой книге Тирион плыл с Джоном Когннингтоном по прозвищу "Грифф". В пользу этой версии говорит и слово "грифф".

Альтернативная тема по большей части посвящена Давосу. Здесь же сильны и признаки Дейнерис Бурерожденной: "ден", "кхал", "дракон", "дрог". Если вглядется, то можно увидеть всего по немножку: "бриен", "ходор", "бран", "виктарион" и прочие. Одной из причин связи линии Давоса и линии Дейнерис может быть "дракон": Драконий Камень, где расположен замок Станниса и настоящие драконы Дени. Тем не менее, основной в этой теме --- Давос, что и подтверждают основные документы ниже. 

```text Документы темы 2
Книга 1, Глава 63, Тирион
Книга 3, Глава 62, Тирион
Книга 1, Глава 39, Тирион
Книга 3, Глава 68, Тирион
Книга 1, Глава 32, Тирион
Книга 1, Глава 22, Тирион
Книга 5, Глава 2, Тирион
Книга 2, Глава 4, Тирион
Книга 5, Глава 23, Тирион
Книга 5, Глава 28, Тирион
```

```text Документы темы 2 "наоборот"
Книга 2, Глава 43, Давос
Книга 2, Глава 59, Давос
Книга 2, Глава 11, Давос
Книга 3, Глава 12, Давос
Книга 3, Глава 38, Давос
Книга 3, Глава 56, Давос
Книга 2, Глава 1, Пролог
Книга 3, Глава 65, Давос
Книга 3, Глава 27, Давос
Книга 5, Глава 16, Давос
Книга 5, Глава 10, Давос
```

### Тема 3. Кхалиси vs ???
<div id="topic_3">
	<svg id='topic_3_pos'></svg>
	<svg id='topic_3_neg'></svg>
</div>
<script type="text/javascript"> 
	topic_clouds('/data/2015-10-01-Game_Of_Thrones_Topic_Modelling/GoT_topic_words_' + 3 + '.csv', 'topic_3_pos', 'topic_3_neg')
</script>

Здесь с положительной темой никаких сомнений нет: сплошная Дейнерис. Отрицательная тема интересней --- основной здесь выступает Кейтелин Старк, но присутствуют и "давос", "робб", "санса", "алейн", "джейм", "джон", "станнис" и т.д. И если присутствие большинства из них вполне объяснимо, то персонажи линии Станниса ("давос", "станнис", "мелисандр" объяснить сложно. Примечательно так же, что хоть Санса поменяла имя, связь с родственниками не потеряла :)

```text Документы темы 3
Книга 1, Глава 65, Дейенерис
Книга 3, Глава 25, Дейенерис
Книга 3, Глава 73, Дейенерис
Книга 3, Глава 44, Дейенерис
Книга 3, Глава 59, Дейенерис
Книга 5, Глава 3, Дейенерис
Книга 5, Глава 17, Дейенерис
Книга 1, Глава 47, Дейенерис
Книга 1, Глава 24, Дейенерис
Книга 3, Глава 10, Дейенерис
```

```text Документы темы 3 "наоборот"
Книга 4, Глава 42, Алейна
Книга 4, Глава 24, Алейна
Книга 4, Глава 11, Санса
Книга 2, Глава 43, Давос
Книга 1, Глава 60, Кейтилин
Книга 1, Глава 35, Кейтилин
Книга 3, Глава 51, Кейтилин
Книга 2, Глава 40, Кейтилин
Книга 1, Глава 72, Кейтилин
Книга 3, Глава 21, Тирион
Документы 
```

### Тема 4. Контрабандист vs Ходор и Бран

<div id="topic_4">
	<svg id='topic_4_pos'></svg>
	<svg id='topic_4_neg'></svg>
</div>
<script type="text/javascript"> 
	topic_clouds('/data/2015-10-01-Game_Of_Thrones_Topic_Modelling/GoT_topic_words_' + 4 + '.csv', 'topic_4_pos', 'topic_4_neg')
</script>


"Положительная" тема здесь --- еще одна тема Давоса. Однако, здесь помимо Давоса выделяются "сэм" и "тирион". Если с первым все объяснимо --- Станнис довольно долго гостил на Стене у ночного дозора, то с Тирионом найти объяснение нелегко. Возможно, здесь роль сыграла битва при Черноводной: благо что "черноводн" среди списка слов встречается.

"Отрицательная" тема же здесь проста --- практически все в ней относится к Ходору с Браном. 

Список документов в данном случае ничего интересно не привноситю 

```text Документы темы 4
Книга 2, Глава 43, Давос
Книга 2, Глава 59, Давос
Книга 2, Глава 11, Давос
Книга 3, Глава 12, Давос
Книга 3, Глава 38, Давос
Книга 3, Глава 56, Давос
Книга 3, Глава 65, Давос
Книга 3, Глава 27, Давос
Книга 5, Глава 16, Давос
Книга 2, Глава 1, Пролог
Книга 5, Глава 10, Давос
Книга 3, Глава 7, Давос
```

```text Документы темы 4 "наоборот"
Книга 3, Глава 58, Бран
Книга 1, Глава 38, Бран
Книга 1, Глава 54, Бран
Книга 2, Глава 17, Бран
Книга 1, Глава 25, Бран
Книга 5, Глава 35, Бран
Книга 3, Глава 42, Бран
Книга 2, Глава 70, Бран
Книга 3, Глава 11, Бран
Книга 5, Глава 14, Бран
```

### Тема 5. Инвалиды??? vs Джейм и Бриенна

<div id="topic_5">
	<svg id='topic_5_pos'></svg>
	<svg id='topic_5_neg'></svg>
</div>
<script type="text/javascript"> 
	topic_clouds('/data/2015-10-01-Game_Of_Thrones_Topic_Modelling/GoT_topic_words_' + 5 + '.csv', 'topic_5_pos', 'topic_5_neg')
</script>
 
Здесь "положительная" тема вновь похожа на винегрет: с одной стороны, лидирует "бран", а с другой по пятам за ним следуют "тирион" и "давос". 

"Отрицательная" тема, опять же, проста и понятна: ее документы относятся к Джейме Ланнистеру и Бриенне Тарт c легким оттенком Сансы Старк (Алейны Стоун), что понятно.

Документы этих тем:


```text Документы темы 5
Книга 3, Глава 58, Бран
Книга 1, Глава 25, Бран
Книга 1, Глава 54, Бран
Книга 3, Глава 42, Бран
Книга 1, Глава 38, Бран
Книга 2, Глава 17, Бран
Книга 5, Глава 35, Бран
Книга 2, Глава 70, Бран
Книга 5, Глава 14, Бран
Книга 1, Глава 63, Тирион
```

```text Документы темы 5 "наоборот"
Книга 4, Глава 5, Бриенна
Книга 4, Глава 10, Бриенна
Книга 4, Глава 21, Бриенна
Книга 4, Глава 28, Джейме
Книга 4, Глава 15, Бриенна
Книга 4, Глава 42, Алейна
Книга 4, Глава 43, Бриенна
Книга 3, Глава 46, Джейме
Книга 3, Глава 69, Джейме
Книга 4, Глава 34, Джейме
```


### Тема 6. Сэм Тарли vs Джон Сноу

<div id="topic_6">
	<svg id='topic_6_pos'></svg>
	<svg id='topic_6_neg'></svg>
</div>
<script type="text/javascript"> 
	topic_clouds('/data/2015-10-01-Game_Of_Thrones_Topic_Modelling/GoT_topic_words_' + 6 + '.csv', 'topic_6_pos', 'topic_6_neg')
</script>

Эта тема довольно интересна: в отличие от предыдущих здесь Сэм и Джон встречаются не вместе, а напротив, противопоставляются друг другу! Так, документы "положительной" относятся к похождениям и мыслям Сэма (и чуточку Брана), а "отрицательная" подтема полностью относится к Джону Сноу. 


Документы 

```text Документы темы 6
Книга 3, Глава 20, Сэмвел
Книга 4, Глава 27, Сэмвел
Книга 3, Глава 48, Сэмвел
Книга 3, Глава 35, Сэмвел
Книга 4, Глава 6, Сэмвел
Книга 4, Глава 36, Сэмвел
Книга 4, Глава 46, Сэмвел
Книга 4, Глава 16, Сэмвел
Книга 3, Глава 58, Бран
Книга 3, Глава 77, Сэмвел
```

```text Документы темы 6 "наоборот"
Книга 3, Глава 17, Джон
Книга 3, Глава 75, Джон
Книга 3, Глава 57, Джон
Книга 3, Глава 43, Джон
Книга 6, Глава 17, Джон
Книга 6, Глава 22, Джон
Книга 3, Глава 9, Джон
Книга 1, Глава 20, Джон
Книга 3, Глава 71, Джон
Книга 3, Глава 66, Джон
Книга 3, Глава 28, Джон
```

### Тема 7. Петир и Санса vs Джейме и Бриенна

<div id="topic_7">
	<svg id='topic_7_pos'></svg>
	<svg id='topic_7_neg'></svg>
</div>
<script type="text/javascript"> 
	topic_clouds('/data/2015-10-01-Game_Of_Thrones_Topic_Modelling/GoT_topic_words_' + 7 + '.csv', 'topic_7_pos', 'topic_7_neg')
</script>

Эта тема так же легко интерпретируема. "Положительная" часть относится к главам главам Сансы Старк (Алены Стоун) и Петира Бейлиша --- к этому и "лиза" и "роберт" и "нестор".
"Отрицательная" же --- еще одна относящаяся к Джейме и Бриенне, но на этот раз без примеси Сансы, Сэма и прочих.

```text Документы темы 7
Книга 4, Глава 42, Алейна
Книга 4, Глава 24, Алейна
Книга 4, Глава 11, Санса
Книга 3, Глава 70, Санса
Книга 3, Глава 82, Санса
Книга 1, Глава 16, Санса
Книга 3, Глава 8, Санса
Книга 1, Глава 52, Санса
Книга 3, Глава 30, Санса
Книга 1, Глава 35, Кейтилин
```

```text Документы темы 7 "наоборот"
Книга 4, Глава 21, Бриенна
Книга 4, Глава 10, Бриенна
Книга 3, Глава 46, Джейме
Книга 4, Глава 5, Бриенна
Книга 4, Глава 28, Джейме
Книга 3, Глава 3, Джейме
Книга 4, Глава 43, Бриенна
Книга 4, Глава 15, Бриенна
Книга 3, Глава 13, Джейме
Книга 3, Глава 23, Джейме
```




# Заключение

Мы рассмотрели теорию и практику тематического моделирования, а точнее метода LSA. Более того, мы применили его на тексте книг серии "Песнь Льда и Пламени" и получили очень даже неплохой результат! Действительно: рассмотренные нами темы естественно интерпретируются и довольно-таки неплохо описывают взаимодействия персонажей игры престолов. Еще более удивительно, что имена персонажей оказываются наиболее важными для определения тем, впрочем возможные причиные этого мы обсудили. 

Как же на практике использовать тематическое моделирование? Во-первых, наиболее очевидное применение --- для интерпретиации текста. Допустим мы не читали киниги и вообще ничего не слышали про Игру Престолов, тогда применив LSA к исходным текстам книг ПЛиО мы, выделив основные темы, узнали основных персонажей, их взаимосвязь с друг-другом, а так же связанные с ними понятия (например, слова соответствующие местности). Во-вторых, получившееся представление документов в виде векторов в пространстве тем $$u_{\cdot}^{(d)}$$ можно использовать для дальнейшего анализа в других алгоритмах. Например, для решения более сложных задач машинного обучения: кластеризации документов, их классификации и т.д.

Если вас заинтересовал тема тем (прошу прощения за дурной каламбур) в "Песни Льда и Пламени", то по [ссылке](https://gist.github.com/Obus/059805567893ba70dc22) доступны по 20 наиболее важных слов для первых 150 тем. Если же хочется поиграться с темами самостоятельно, то в качестве отправных точек могу посоветовать следующее

1. [IPython Notebook](не готово еще) с кодом, используемым для этой статье, и полученными результатами.
2. Python пакет [gensim](https://radimrehurek.com/gensim/), содержащий как вспомогательный инструменты для создания корпуса, реализацию LSA, так и реализации гораздо более сложных, но и интересных методов тематического моделирования
3. [Статья на machinelearning.ru](http://www.machinelearning.ru/wiki/index.php?title=%D0%A2%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5)

Надеюсь, что было интересно :) В следующих статьях, возможно, мы продолжим тему автоматического анализа текстов Игры Престолов и постараемся вытащить из текстов книг ПЛиО что-нибудь менее тривиальное, чем факт знакомства Джона Сноу и Сэма Тарли. Если вам это направление интересно --- смело лайкайте эту статью во всех возможных соцсетях :)

До встречи!




